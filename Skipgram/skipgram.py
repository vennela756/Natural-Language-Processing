# -*- coding: utf-8 -*-
"""skipgram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mT_0k-48meY6UXnDQ-abJnDqJ76B0JVi
"""

# loading libraries
import numpy as np
import nltk

class skipgram(object):
    def __init__(self):
        self.N = 15
        self.V = 0
        self.W1 = []
        self.W2 = []
        self.alpha = 0.02
        self.window_size = 2
        self.epochs = 350
        self.word2index = {}
        self.index2word = []
        

    def data_generator(self,raw_data):
      word_count = {}
      word2index = {}
      for sentence in raw_data:
        for word in sentence:
          if word in word_count:
            word_count[word] += 1
          else:
            word_count[word] = 1

      self.V = len(word_count)    #######
      self.index2word = list(word_count.keys())
      self.word2index = dict((word, i) for i, word in enumerate(self.index2word))
      #print(self.word2index)
      #print(self.index2word)
      training_data = []
      for sentence in raw_data:
        for i,word in enumerate(sentence):
          target_word = self.word2onehot(word)
          context_words = []
          for j in range(i-self.window_size,i+self.window_size+1):
            if ((j>=0 and j<len(sentence)) and j!=i):
              ctxt_word = self.word2onehot(sentence[j])
              context_words.append(ctxt_word)
              #print(word,sentence[j])
          if(len(context_words) != 0 ):
            training_data.append([target_word,context_words])


      return np.array(training_data)

    def word2onehot(self,word):
      word_vec = [0 for i in range(self.V)] 
      word_vec[self.word2index[word]] = 1
      return np.array(word_vec)

      
    def softmaxlayer(self,X):
        temp = np.exp(X)
        result = temp/(np.sum(temp))
        return result

    def feed_forward(self,X):
        # dimesion of X is Vx1
        self.hidden = self.W1.T @ X
        self.output = self.W2.T @ self.hidden
        self.y_hat = self.softmaxlayer(self.output) 
        return self.y_hat,self.hidden,self.output
          
    def backpropagation(self,E,h,target_word):
        temp = self.W2 @ E
        derv1 = target_word.reshape(self.V,1) @ temp.T.reshape(1,self.N)
        derv2 = self.hidden.T.reshape(self.N,1) @ E.reshape(1,self.V)

        self.W1 = self.W1 - self.alpha*derv1 # 
        self.W2 = self.W2 - self.alpha*derv2
          
    def train(self,training_data):

        #Initialise weights
        self.W1 = np.random.randn(self.V,self.N)
        self.W2 = np.random.randn(self.N,self.V)

        loss1 = 0
        loss2 = 0
        for epoch in range(self.epochs):       
            self.loss = 0
            for target_word,context_words in training_data:
              #print(target_word,context_words)
              y_pred,h,u = self.feed_forward(target_word) #VX1
              
              E = np.sum([np.subtract(y_pred,word) for word in context_words],axis=0) # VX1   # from cross-entropy=> delta(j)*output => for weight change rule
              self.backpropagation(E, h, target_word)
              #calculate loss
              self.loss += -np.sum([u[np.where(word == 1)[0]] for word in context_words]) + len(context_words) * np.log(np.sum(np.exp(u)))
            loss2 = self.loss
            print('\nEpoch:', epoch, 'Loss: ', self.loss)
            if(np.abs(loss2 -loss1) < 0.01):
              break
            loss1 = loss2
            


    def word_emb(self,word):
      return self.W1[self.word2index[word]]
             
    def predict(self,word,n):
      word1 = self.word_emb(word)
      word_similarity = {}
      sum = 0
      for i in range(self.V):
        word2 = self.W1[i]
        numerator = np.dot(word1,word2)
        denominator = np.linalg.norm(word1)*np.linalg.norm(word2)
        similarity = numerator/denominator
        word_similarity[self.index2word[i]] = similarity
        #sum += np.linalg.norm(word1)*np.linalg.norm(word2)
      top_words = sorted(word_similarity.items(), key = lambda x : x[1], reverse = True)
      print()
      for word,similarity in top_words[:n]:
        print(word,similarity)

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from string import punctuation
import re

from sklearn.decomposition import PCA

from matplotlib import pyplot as plt
import plotly.graph_objects as go

import numpy as np

import warnings
warnings.filterwarnings('ignore')

nltk.download('stopwords')

fi = open("sentences.txt",'r') # change self.N also
f = fi.read()
#f = "One barking blue dog, ,sets all street? a-barking. A staff is quickly found to beat a blue dog with. happened to be a blue dog "

#f = ""
#for fileid in webtext.fileids():
#  f =  webtext.raw(fileid)

stop_words = nltk.corpus.stopwords.words('english') 
remove_terms = punctuation + '0123456789'
#print(remove_terms)
wpt = nltk.WordPunctTokenizer()

animals = ["dog","cat","horse","cow","fox","zebra","elephant","goat", "tiger", "bullock"]  # 10 words
birds = ["cuckoo","parrot","crow","sparrow","peacock","eagle","pigeon","hawk","owl","duck"]
raw_data = []
# iterate through each sentence in the file
for i in sent_tokenize(f):
    temp = [] 
    # tokenize the sentence into words
    sentence = ''.join((filter(lambda x: x not in list(remove_terms), i)))
    sentence = sentence.strip()
    sentence = sentence.lower()
    #want to remove digits and punctuation??????
    word_list = word_tokenize(sentence)
    #print(word_list)
    if ( any(animal in word_list for animal in animals) or any(bird in word_list for bird in birds) ):
      for j in word_list:
          if j not in remove_terms:
            if j not in stop_words:
              temp.append(j.lower())
      raw_data.append(temp)

#print(raw_data)

w2v = skipgram()
#generate data =>  raw_data ->> training_data
training_data = w2v.data_generator(raw_data)
#print(len(training_data))   ###removed 0 sentence words
#print("change self.N also")
# train word2vec model
w2v.train(training_data)

w2v.predict("cow",5)

animals_vec = []
for animal in animals:
  animals_vec.append(w2v.word_emb(animal))
a_len = len(animals_vec)
animals_vec = np.array(animals_vec)

birds_vec = []
for bird in birds:
  birds_vec.append(w2v.word_emb(bird))
b_len = len(birds_vec)
birds_vec = np.array(birds_vec)

#Intra cluster distance => centroid_diameter_dist
I1 = np.sum(animals_vec,axis=0)/a_len
sum = 0
for a in animals_vec:
  sum += np.linalg.norm(a-I1)
a_intra_dist = 2*sum/a_len
print("Animal Cluster => centroid_diameter_dist = ", a_intra_dist)



#Intra cluster distances MAX,MIN,AVG
vals = []
for i in range(0,a_len):
  for j in range(i+1,a_len):
    vals.append(np.linalg.norm(animals_vec[i]-animals_vec[j]))

print("MAX Intra Cluster Distance for animals = ", max(vals))
print("MIN Intra Cluster Distance for animals = ", min(vals))
print("AVG Intra Cluster Distance for animals = ", np.sum(vals)/len(vals))

print()

I2 = np.sum(birds_vec,axis=0)/b_len
sum = 0
for b in birds_vec:
  sum += np.linalg.norm(b-I2)
b_intra_dist = 2*sum/b_len
print("Bird Cluster => centroid_diameter_dist = ", b_intra_dist)

vals = []
for i in range(0,b_len):
  for j in range(i+1,b_len):
    vals.append(np.linalg.norm(birds_vec[i]-birds_vec[j]))

print("MAX Intra Cluster Distance for birds = ", max(vals))
print("MIN Intra Cluster Distance for birds = ", min(vals))
print("AVG Intra Cluster Distance for birds = ", np.sum(vals)/len(vals))


print()
print()
#Inter cluster distance => avg_linkage_dist
sum = 0
for a in animals_vec:
  for b in birds_vec:
    sum += np.linalg.norm(a-b)
avg_linkage_dist = sum/(a_len*b_len)
print("inter-cluster distance => avg_linkage_distance = ", avg_linkage_dist)


#Inter cluster distances MAX,MIN,AVG
vals = []
for i in range(0,a_len):
  for j in range(0,b_len):
    vals.append(np.linalg.norm(animals_vec[i]-birds_vec[j]))

print("MAX Inter Cluster Distance for animals-birds = ", max(vals))
print("MIN Inter Cluster Distance for animals-birds = ", min(vals))
print("AVG Inter Cluster Distance for animals-birds = ", np.sum(vals)/len(vals))

# Advanced Parameters

# Davies-Bouldin index (DB):

M = np.linalg.norm(I1-I2)
S1 = np.sqrt(np.mean((animals_vec - I1)**2))
S2 = np.sqrt(np.mean((birds_vec - I2)**2))

DB = (S1+S2)/M
print("Davies-Bouldin index (DB) = ",DB)

# Dunn index: 

X_dots = (animals_vec*animals_vec).sum(axis=1).reshape((a_len,1))*np.ones(shape=(1,a_len))
X = -2*animals_vec.dot(animals_vec.T)
A_D = 2*X_dots - X

X_dots = (birds_vec*birds_vec).sum(axis=1).reshape((a_len,1))*np.ones(shape=(1,a_len))
X = -2*birds_vec.dot(birds_vec.T)
B_D = 2*X_dots - X
M = M*M
intra_max_value = np.sqrt(max(np.max(A_D),np.max(B_D)))
dunn = M/intra_max_value
print("Dunn index = ",dunn)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

x = np.append(animals_vec,birds_vec,axis=0) 
y = np.append(np.ones(a_len,dtype=int),np.zeros(b_len,dtype=int))

pca = PCA(n_components=2)
t = pca.fit_transform(x)


plt.scatter(t[:,0][y==1], t[:,1][y==1], c='b',label="Animals") # animals
plt.scatter(t[:,0][y==0], t[:,1][y==0], c='r',label="Birds") # birds


for i,txt in enumerate(animals):
  plt.annotate(txt,(t[:,0][y==1][i],t[:,1][y==1][i]))

for i,txt in enumerate(birds):
  plt.annotate(txt,(t[:,0][y==0][i],t[:,1][y==0][i]))
plt.show()