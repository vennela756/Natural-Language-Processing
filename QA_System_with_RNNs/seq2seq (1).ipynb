{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XSK9hHkfcfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7740394-9876-4657-f487-b0c0a3b8dd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_dir = \"/content/drive/My Drive/QA/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq7nfPz1gA-u"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "f = open(project_dir+\"train-v2.0.json\",mode=\"r\",encoding=\"utf-8\")\n",
        "raw_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsGGKrtgm-ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c3ea00-eb0a-4871-e727-d0d2ab700a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IquHxJXNhJu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3c52e6-c437-45d1-8e4c-b1c4b1d29aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Pre-process the data\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def pre_process(raw_data,max_input_samples,max_context_length,max_question_length,max_answer_length):\n",
        "  \"\"\"keys = {\n",
        "        \"version\":\n",
        "        \"data\":[{\n",
        "          \"title\":\n",
        "          \"paragraphs\":[{\n",
        "                  \"qas\":[{\"question\":\n",
        "              \"answers\":\n",
        "              }]\n",
        "            \"context\":\n",
        "          }]\n",
        "\n",
        "        }]\n",
        "             }  \"\"\"\n",
        "  \"Fill here\"\n",
        "  data = []\n",
        "  data_tokenized = []\n",
        "  data1 = raw_data[\"data\"]\n",
        "  count = 0\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  for data_ele in data1:\n",
        "    for para in data_ele[\"paragraphs\"]:\n",
        "      qas = para[\"qas\"]\n",
        "      context = [k.lower() for k in nltk.word_tokenize(para[\"context\"])]\n",
        "      #context_tokenized = tokenizer(para[\"context\"], padding = True)\n",
        "      if len(context) <= max_context_length:\n",
        "        for qa in qas:\n",
        "          que = [k.lower() for k in nltk.word_tokenize(qa[\"question\"])]\n",
        "          #que_tokenized = tokenizer(qa[\"question\"], padding = True)\n",
        "          if len(qa[\"answers\"]) == 1:\n",
        "            ans = [k.lower() for k in nltk.word_tokenize(qa[\"answers\"][0][\"text\"])]\n",
        "            ans_start = qa[\"answers\"][0][\"answer_start\"]\n",
        "            answer_indices = [ans_start, ans_start+len(ans)]\n",
        "            if \"how\" not in que and \"why\" not in que:\n",
        "              if len(que) <= max_question_length and len(que) <= max_answer_length:\n",
        "                if count <= max_input_samples:\n",
        "                  data.append([context,que,ans])\n",
        "                  contexts.append(para[\"context\"])\n",
        "                  questions.append(qa[\"question\"])\n",
        "                  #data_tokenized.append([context_tokenized,que_tokenized,answer_indices])\n",
        "                  count = count + 1\n",
        "                else:\n",
        "                  break\n",
        "    if count >  max_input_samples:\n",
        "      break\n",
        "  return data, contexts, questions, answer_indices\n",
        "\n",
        "max_input_samples = 600\n",
        "max_context_length = 300\n",
        "max_question_length = 50\n",
        "max_answer_length = 50\n",
        "\n",
        "data, contexts, questions, answer_indices = pre_process(raw_data,max_input_samples,max_context_length,max_question_length,max_answer_length)\n",
        "contexts_tokenized = tokenizer(contexts, padding = True, truncation= True, return_tensors = \"pt\")\n",
        "questions_tokenized = tokenizer(questions, padding= True, truncation = True, return_tensors = \"pt\")\n",
        "\n",
        "contexts_tokenized = contexts_tokenized[\"input_ids\"]\n",
        "questions_tokenized = questions_tokenized[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J5C5N3UkweJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "b712d398-a4db-4530-c86c-70cc99d0b7ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  1., 200., 321.,  71.,   6.,   2.,   0.,   0.,   0.,   0.]),\n",
              " array([ 0,  5, 10, 15, 20, 25, 30, 35, 40, 45, 50]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## data distribution\n",
        "what_count = 0\n",
        "who_count = 0\n",
        "when_count = 0\n",
        "where_count = 0\n",
        "ques_length_list = []\n",
        "for one_set in data:\n",
        "  ques_length = 0 \n",
        "  for ques_word in one_set[1]:\n",
        "    ques_length = ques_length + 1\n",
        "    if ques_word == 'when':\n",
        "      when_count = when_count + 1\n",
        "    elif ques_word == 'what':\n",
        "      what_count = what_count + 1\n",
        "    elif ques_word == 'who':\n",
        "      who_count = who_count + 1\n",
        "    elif ques_word == 'where':\n",
        "      where_count = where_count + 1\n",
        "  ques_length_list.append(ques_length)\n",
        "#print(when_count)\n",
        "#print(who_count)\n",
        "#print(what_count)\n",
        "#print(where_count)\n",
        "#print(ques_length_list)\n",
        "\n",
        "ques_data = {}\n",
        "ques_data['when'] = when_count\n",
        "ques_data['what'] = what_count\n",
        "ques_data['who'] = who_count\n",
        "ques_data['where'] = where_count\n",
        "names = list(ques_data.keys())\n",
        "values = list(ques_data.values())\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 7))\n",
        "axs[0].bar(names, values)\n",
        "axs[1].hist(ques_length_list, bins = [i*5 for i in range(11)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZiTKTJXyqkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e034654-8d7b-405a-fa1c-52c93c2afbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 487, 879, 880, 881, 135, 130, 77, 2, 520, 27, 35, 73, 122, 2, 265, 2, 160, 348, 6, 277, 4, 135, 6, 288, 5, 165, 2, 289, 2, 16, 66, 5, 410, 152, 6, 541, 591, 19, 9, 37, 2, 6, 882, 12, 310, 5, 3, 521, 411, 19, 146, 122, 7, 883, 44, 11, 37, 4, 853, 23, 10, 224, 2, 212, 156, 2, 3, 63, 88, 39, 7, 3, 64, 11, 290, 202, 505, 7, 82, 62, 4, 31, 267, 233, 3, 119, 7, 8, 11, 218, 22, 2, 166, 5, 38, 259, 2, 25, 506, 10, 19, 9, 57, 51, 129, 2, 188, 147, 76, 47, 6, 131, 3, 65, 132, 67, 170, 80, 161, 5, 38, 6, 507, 522, 4, 21, 59, 45, 40, 453, 299, 470, 20]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n",
        "\n",
        "\n",
        "def in_white_list(_word):\n",
        "    valid_word = False\n",
        "    for char in _word:\n",
        "        if char in WHITELIST:\n",
        "            valid_word = True\n",
        "            break\n",
        "    if valid_word is False:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "max_target_vocab_size = 5000\n",
        "max_input_vocab_size = 5000\n",
        "\n",
        "data_set = raw_data[\"data\"]\n",
        "input_data_samples = []\n",
        "output_data_samples = []\n",
        "\n",
        "input_max_seq_length = 0\n",
        "target_max_seq_length = 0\n",
        "\n",
        "input_counter = Counter()\n",
        "target_counter = Counter()\n",
        "\n",
        "input_data_samples = []\n",
        "output_data_samples = []\n",
        "\n",
        "data_set1 = []\n",
        "count = 0\n",
        "for sample in data_set:\n",
        "  for para in sample[\"paragraphs\"]:\n",
        "      qas = para[\"qas\"]\n",
        "      context = [k.lower() for k in nltk.word_tokenize(para[\"context\"])]\n",
        "      if len(context) <= max_context_length:\n",
        "        for qa in qas:\n",
        "          que = [k.lower() for k in nltk.word_tokenize(qa[\"question\"])]\n",
        "          if len(qa[\"answers\"]) == 1:\n",
        "            if \"how\" not in que and \"why\" not in que:\n",
        "              if len(que) <= max_question_length and len(que) <= max_answer_length:\n",
        "                if count <= max_input_samples:\n",
        "                  data_set1.append([para[\"context\"],qa[\"question\"],qa[\"answers\"][0][\"text\"]])\n",
        "                  count = count + 1\n",
        "                else:\n",
        "                  break\n",
        "  if count > max_input_samples:\n",
        "    break\n",
        "\n",
        "for sample in data_set1:\n",
        "    #print(sample)\n",
        "    paragraph, question, answer = sample\n",
        "    paragraph_word_list = [w.lower() for w in nltk.word_tokenize(paragraph) if in_white_list(w)]\n",
        "    question_word_list = [w.lower() for w in nltk.word_tokenize(question) if in_white_list(w)]\n",
        "    answer_word_list = [w.lower() for w in nltk.word_tokenize(answer) if in_white_list(w)]\n",
        "\n",
        "    input_data = paragraph_word_list + ['Q'] + question_word_list\n",
        "    output_data = ['START'] + answer_word_list + ['END']\n",
        "\n",
        "    input_data_samples.append(input_data)\n",
        "    output_data_samples.append(output_data)\n",
        "\n",
        "    for w in input_data:\n",
        "        input_counter[w] += 1\n",
        "    for w in output_data:\n",
        "        target_counter[w] += 1\n",
        "\n",
        "    input_max_seq_length = max(input_max_seq_length, len(input_data))\n",
        "    target_max_seq_length = max(target_max_seq_length, len(output_data))\n",
        "\n",
        "input_word2idx = dict()\n",
        "target_word2idx = dict()\n",
        "for idx, word in enumerate(input_counter.most_common(max_input_vocab_size)):\n",
        "    input_word2idx[word[0]] = idx + 2\n",
        "for idx, word in enumerate(target_counter.most_common(max_target_vocab_size)):\n",
        "    target_word2idx[word[0]] = idx + 1\n",
        "\n",
        "target_word2idx['UNK'] = 0\n",
        "input_word2idx['PAD'] = 0\n",
        "input_word2idx['UNK'] = 1\n",
        "\n",
        "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "num_input_tokens = len(input_idx2word)\n",
        "num_target_tokens = len(target_idx2word)\n",
        "\n",
        "input_encoded_data_samples = []\n",
        "target_encoded_data_samples = []\n",
        "\n",
        "for input_data, output_data in zip(input_data_samples, output_data_samples):\n",
        "    input_encoded_data = []\n",
        "    target_encoded_data = []\n",
        "    for word in input_data:\n",
        "        if word in input_word2idx:\n",
        "            input_encoded_data.append(input_word2idx[word])\n",
        "        else:\n",
        "            input_encoded_data.append(1)\n",
        "    for word in output_data:\n",
        "        if word in target_word2idx:\n",
        "            target_encoded_data.append(target_word2idx[word])\n",
        "        else:\n",
        "            target_encoded_data.append(0)\n",
        "    input_encoded_data_samples.append(input_encoded_data)\n",
        "    target_encoded_data_samples.append(target_encoded_data)\n",
        "\n",
        "samples = [input_encoded_data_samples, target_encoded_data_samples]\n",
        "print(samples[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyCr2egdsXD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd936008-e822-4224-ee3c-22d0c0906d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "274\n"
          ]
        }
      ],
      "source": [
        "print(input_max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBMSA-N_reP6"
      },
      "outputs": [],
      "source": [
        "def generate_batch(input_data, target_data, batch_size):\n",
        "    num_batches = len(input_data) // batch_size\n",
        "\n",
        "    while True:\n",
        "        for batchIdx in range(0, num_batches):\n",
        "            start = batchIdx * batch_size\n",
        "            end = (batchIdx + 1) * batch_size\n",
        "            encoder_input_data_batch = pad_sequences(input_data[start:end], input_max_seq_length)\n",
        "            decoder_target_data_batch = np.zeros(shape=(batch_size, target_max_seq_length,\n",
        "                                                        num_target_tokens))\n",
        "            decoder_input_data_batch = np.zeros(shape=(batch_size, target_max_seq_length,\n",
        "                                                       num_target_tokens))\n",
        "            for lineIdx, target_wid_list in enumerate(target_data[start:end]):\n",
        "                for idx, wid in enumerate(target_wid_list):\n",
        "                    if wid == 0:  # UNKNOWN\n",
        "                        continue\n",
        "                    decoder_input_data_batch[lineIdx, idx, wid] = 1\n",
        "                    if idx > 0:\n",
        "                        decoder_target_data_batch[lineIdx, idx - 1, wid] = 1\n",
        "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pncan2dC8ZId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bc9d5b-c085-4130-c34d-6c85597484b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 256)    609792      ['encoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None, 587)]  0           []                               \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)            [(None, 256),        525312      ['encoder_embedding[0][0]']      \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 256),  864256      ['decoder_inputs[0][0]',         \n",
            "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
            "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, None, 256)    0           ['decoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)          (None, None, 587)    150859      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,150,219\n",
            "Trainable params: 2,150,219\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create a model\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Dropout, add, RepeatVector\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "def create_model(num_input_tokens, input_max_seq_length, num_target_tokens):\n",
        "        hidden_units = 256\n",
        "\n",
        "        encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "        encoder_embedding = Embedding(input_dim=num_input_tokens, output_dim=hidden_units,\n",
        "                                      input_length=input_max_seq_length, name='encoder_embedding')\n",
        "        \n",
        "        encoder_lstm = LSTM(units=hidden_units, return_state=True, name='encoder_lstm')\n",
        "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "        encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "        decoder_inputs = Input(shape=(None, num_target_tokens), name='decoder_inputs')\n",
        "        decoder_lstm = LSTM(units=hidden_units, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                         initial_state=encoder_states)\n",
        "        decoder_dense = Dense(units=num_target_tokens, activation='softmax', name='decoder_dense')\n",
        "        decoder_outputs = Dropout(0.3)(decoder_outputs)\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        #opt = SGD(lr=0.01)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "\n",
        "        encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_state_inputs = [Input(shape=(hidden_units,)), Input(shape=(hidden_units,))]\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
        "        decoder_states = [state_h, state_c]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "        return model, encoder_model, decoder_model\n",
        "\n",
        "model, encoder_model, decoder_model = create_model(num_input_tokens, input_max_seq_length, num_target_tokens)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPvvGInj7jah"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(samples[0], samples[1], test_size=0.2,random_state=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHRBygT091IN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddbc0b3-d6c0-496c-f9a9-f9ab89e5ae1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 274)\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "batch_size = 20\n",
        "train_gen = generate_batch(x_train, y_train, batch_size)\n",
        "test_gen = generate_batch(x_test, y_test, batch_size)\n",
        "a = next(train_gen)\n",
        "a = next(train_gen)\n",
        "\n",
        "b = a[0][0]\n",
        "r = (len(b),len(b[0]))\n",
        "print(r)\n",
        "train_num_batches = len(x_train) // batch_size\n",
        "test_num_batches = len(x_test) // batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UYOudxD0dsEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fd7201-b1ff-4f56-815d-18e98c0032f6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.0569\n",
            "Epoch 1: val_loss improved from inf to 0.90907, saving model to /content/drive/My Drive/QA/seq2seq200.h5\n",
            "24/24 [==============================] - 29s 1s/step - loss: 1.0567 - accuracy: 0.0569 - val_loss: 0.9091 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9984 - accuracy: 0.0587\n",
            "Epoch 2: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9984 - accuracy: 0.0587 - val_loss: 0.9172 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9945 - accuracy: 0.0588\n",
            "Epoch 3: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.9945 - accuracy: 0.0588 - val_loss: 0.9243 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9928 - accuracy: 0.0591\n",
            "Epoch 4: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.9928 - accuracy: 0.0591 - val_loss: 0.9308 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9942 - accuracy: 0.0589\n",
            "Epoch 5: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9942 - accuracy: 0.0589 - val_loss: 0.9383 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9905 - accuracy: 0.0607\n",
            "Epoch 6: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.9905 - accuracy: 0.0607 - val_loss: 0.9434 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9852 - accuracy: 0.0609\n",
            "Epoch 7: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.9852 - accuracy: 0.0609 - val_loss: 0.9499 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9851 - accuracy: 0.0615\n",
            "Epoch 8: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.9851 - accuracy: 0.0615 - val_loss: 0.9557 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9790 - accuracy: 0.0619\n",
            "Epoch 9: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.9790 - accuracy: 0.0619 - val_loss: 0.9618 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9792 - accuracy: 0.0616\n",
            "Epoch 10: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.9792 - accuracy: 0.0616 - val_loss: 0.9645 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9738 - accuracy: 0.0619\n",
            "Epoch 11: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.9738 - accuracy: 0.0619 - val_loss: 0.9834 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9628 - accuracy: 0.0619\n",
            "Epoch 12: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.9628 - accuracy: 0.0619 - val_loss: 0.9764 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9607 - accuracy: 0.0620\n",
            "Epoch 13: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.9607 - accuracy: 0.0620 - val_loss: 0.9817 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9575 - accuracy: 0.0614\n",
            "Epoch 14: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9575 - accuracy: 0.0614 - val_loss: 0.9869 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.0623\n",
            "Epoch 15: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9499 - accuracy: 0.0623 - val_loss: 0.9987 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9486 - accuracy: 0.0619\n",
            "Epoch 16: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.9486 - accuracy: 0.0619 - val_loss: 1.0018 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9352 - accuracy: 0.0620\n",
            "Epoch 17: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.9352 - accuracy: 0.0620 - val_loss: 1.0047 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9273 - accuracy: 0.0624\n",
            "Epoch 18: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.9273 - accuracy: 0.0624 - val_loss: 1.0095 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9278 - accuracy: 0.0625\n",
            "Epoch 19: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9278 - accuracy: 0.0625 - val_loss: 1.0091 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9175 - accuracy: 0.0635\n",
            "Epoch 20: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.9175 - accuracy: 0.0635 - val_loss: 1.0146 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9083 - accuracy: 0.0636\n",
            "Epoch 21: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.9083 - accuracy: 0.0636 - val_loss: 1.0189 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.0645\n",
            "Epoch 22: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.9010 - accuracy: 0.0645 - val_loss: 1.0232 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8939 - accuracy: 0.0661\n",
            "Epoch 23: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.8939 - accuracy: 0.0661 - val_loss: 1.0252 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8833 - accuracy: 0.0675\n",
            "Epoch 24: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.8833 - accuracy: 0.0675 - val_loss: 1.0281 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8726 - accuracy: 0.0676\n",
            "Epoch 25: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.8726 - accuracy: 0.0676 - val_loss: 1.0278 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8637 - accuracy: 0.0690\n",
            "Epoch 26: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.8637 - accuracy: 0.0690 - val_loss: 1.0340 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8521 - accuracy: 0.0708\n",
            "Epoch 27: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.8521 - accuracy: 0.0708 - val_loss: 1.0325 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8408 - accuracy: 0.0714\n",
            "Epoch 28: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.8408 - accuracy: 0.0714 - val_loss: 1.0284 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8314 - accuracy: 0.0738\n",
            "Epoch 29: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.8314 - accuracy: 0.0738 - val_loss: 1.0393 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.0748\n",
            "Epoch 30: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.8210 - accuracy: 0.0748 - val_loss: 1.0332 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.0767\n",
            "Epoch 31: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.8070 - accuracy: 0.0767 - val_loss: 1.0386 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7939 - accuracy: 0.0794\n",
            "Epoch 32: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 957ms/step - loss: 0.7939 - accuracy: 0.0794 - val_loss: 1.0393 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7909 - accuracy: 0.0797\n",
            "Epoch 33: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 958ms/step - loss: 0.7909 - accuracy: 0.0797 - val_loss: 1.0407 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.0835\n",
            "Epoch 34: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 959ms/step - loss: 0.7732 - accuracy: 0.0835 - val_loss: 1.0416 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.0838\n",
            "Epoch 35: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 956ms/step - loss: 0.7638 - accuracy: 0.0838 - val_loss: 1.0470 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.0855\n",
            "Epoch 36: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 951ms/step - loss: 0.7564 - accuracy: 0.0855 - val_loss: 1.0360 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7407 - accuracy: 0.0880\n",
            "Epoch 37: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 953ms/step - loss: 0.7407 - accuracy: 0.0880 - val_loss: 1.0495 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7375 - accuracy: 0.0890\n",
            "Epoch 38: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 952ms/step - loss: 0.7375 - accuracy: 0.0890 - val_loss: 1.0457 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.0930\n",
            "Epoch 39: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 953ms/step - loss: 0.7212 - accuracy: 0.0930 - val_loss: 1.0402 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7112 - accuracy: 0.0930\n",
            "Epoch 40: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 954ms/step - loss: 0.7112 - accuracy: 0.0930 - val_loss: 1.0473 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.0960\n",
            "Epoch 41: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 958ms/step - loss: 0.7009 - accuracy: 0.0960 - val_loss: 1.0456 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.0973\n",
            "Epoch 42: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 955ms/step - loss: 0.6909 - accuracy: 0.0973 - val_loss: 1.0569 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.0968\n",
            "Epoch 43: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 957ms/step - loss: 0.6877 - accuracy: 0.0968 - val_loss: 1.0438 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.1001\n",
            "Epoch 44: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 955ms/step - loss: 0.6753 - accuracy: 0.1001 - val_loss: 1.0500 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6648 - accuracy: 0.1012\n",
            "Epoch 45: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 956ms/step - loss: 0.6648 - accuracy: 0.1012 - val_loss: 1.0459 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.1032\n",
            "Epoch 46: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 958ms/step - loss: 0.6566 - accuracy: 0.1032 - val_loss: 1.0532 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.1059\n",
            "Epoch 47: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 955ms/step - loss: 0.6476 - accuracy: 0.1059 - val_loss: 1.0561 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.1067\n",
            "Epoch 48: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 958ms/step - loss: 0.6371 - accuracy: 0.1067 - val_loss: 1.0450 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.1075\n",
            "Epoch 49: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 954ms/step - loss: 0.6317 - accuracy: 0.1075 - val_loss: 1.0439 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6207 - accuracy: 0.1102\n",
            "Epoch 50: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 956ms/step - loss: 0.6207 - accuracy: 0.1102 - val_loss: 1.0602 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.1131\n",
            "Epoch 51: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 957ms/step - loss: 0.6134 - accuracy: 0.1131 - val_loss: 1.0432 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.1132\n",
            "Epoch 52: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 957ms/step - loss: 0.6069 - accuracy: 0.1132 - val_loss: 1.0529 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5974 - accuracy: 0.1158\n",
            "Epoch 53: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 957ms/step - loss: 0.5974 - accuracy: 0.1158 - val_loss: 1.0596 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.1154\n",
            "Epoch 54: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.5951 - accuracy: 0.1154 - val_loss: 1.0461 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.1181\n",
            "Epoch 55: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.5822 - accuracy: 0.1181 - val_loss: 1.0564 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.1210\n",
            "Epoch 56: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.5763 - accuracy: 0.1210 - val_loss: 1.0505 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.1196\n",
            "Epoch 57: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.5747 - accuracy: 0.1196 - val_loss: 1.0530 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.1228\n",
            "Epoch 58: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.5656 - accuracy: 0.1228 - val_loss: 1.0503 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5547 - accuracy: 0.1257\n",
            "Epoch 59: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 959ms/step - loss: 0.5547 - accuracy: 0.1257 - val_loss: 1.0588 - val_accuracy: 0.0583 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.1255\n",
            "Epoch 60: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.5455 - accuracy: 0.1255 - val_loss: 1.0623 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.1266\n",
            "Epoch 61: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.5438 - accuracy: 0.1266 - val_loss: 1.0537 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.1259\n",
            "Epoch 62: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.5419 - accuracy: 0.1259 - val_loss: 1.0511 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.1287\n",
            "Epoch 63: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.5278 - accuracy: 0.1287 - val_loss: 1.0524 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.1304\n",
            "Epoch 64: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.5218 - accuracy: 0.1304 - val_loss: 1.0612 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5193 - accuracy: 0.1322\n",
            "Epoch 65: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.5193 - accuracy: 0.1322 - val_loss: 1.0616 - val_accuracy: 0.0583 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.1346\n",
            "Epoch 66: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.5136 - accuracy: 0.1346 - val_loss: 1.0537 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.1314\n",
            "Epoch 67: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.5101 - accuracy: 0.1314 - val_loss: 1.0719 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.1338\n",
            "Epoch 68: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.5063 - accuracy: 0.1338 - val_loss: 1.0692 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.1358\n",
            "Epoch 69: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.4952 - accuracy: 0.1358 - val_loss: 1.0538 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.1373\n",
            "Epoch 70: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 959ms/step - loss: 0.4902 - accuracy: 0.1373 - val_loss: 1.0631 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.1358\n",
            "Epoch 71: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.4861 - accuracy: 0.1358 - val_loss: 1.0723 - val_accuracy: 0.0583 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.1368\n",
            "Epoch 72: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.4840 - accuracy: 0.1368 - val_loss: 1.0720 - val_accuracy: 0.0583 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.1387\n",
            "Epoch 73: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4785 - accuracy: 0.1387 - val_loss: 1.0717 - val_accuracy: 0.0588 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.1388\n",
            "Epoch 74: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.4749 - accuracy: 0.1388 - val_loss: 1.0638 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.1398\n",
            "Epoch 75: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4719 - accuracy: 0.1398 - val_loss: 1.0608 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.1412\n",
            "Epoch 76: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4589 - accuracy: 0.1412 - val_loss: 1.0756 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.1431\n",
            "Epoch 77: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.4589 - accuracy: 0.1431 - val_loss: 1.0795 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.1393\n",
            "Epoch 78: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.4574 - accuracy: 0.1393 - val_loss: 1.0645 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.1426\n",
            "Epoch 79: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4507 - accuracy: 0.1426 - val_loss: 1.0632 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4441 - accuracy: 0.1428\n",
            "Epoch 80: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.4441 - accuracy: 0.1428 - val_loss: 1.0658 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.1441\n",
            "Epoch 81: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4417 - accuracy: 0.1441 - val_loss: 1.0735 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.1419\n",
            "Epoch 82: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4363 - accuracy: 0.1419 - val_loss: 1.0742 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.1450\n",
            "Epoch 83: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.4347 - accuracy: 0.1450 - val_loss: 1.0737 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.1451\n",
            "Epoch 84: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4313 - accuracy: 0.1451 - val_loss: 1.0812 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.1463\n",
            "Epoch 85: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.4290 - accuracy: 0.1463 - val_loss: 1.0773 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.1480\n",
            "Epoch 86: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.4252 - accuracy: 0.1480 - val_loss: 1.0683 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.1473\n",
            "Epoch 87: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.4175 - accuracy: 0.1473 - val_loss: 1.0745 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.1467\n",
            "Epoch 88: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.4200 - accuracy: 0.1467 - val_loss: 1.0704 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.1462\n",
            "Epoch 89: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4194 - accuracy: 0.1462 - val_loss: 1.0659 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.1483\n",
            "Epoch 90: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.4131 - accuracy: 0.1483 - val_loss: 1.0789 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4116 - accuracy: 0.1471\n",
            "Epoch 91: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.4116 - accuracy: 0.1471 - val_loss: 1.0723 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.1488\n",
            "Epoch 92: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.4039 - accuracy: 0.1488 - val_loss: 1.0685 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.1485\n",
            "Epoch 93: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.4049 - accuracy: 0.1485 - val_loss: 1.0717 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.1504\n",
            "Epoch 94: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.3977 - accuracy: 0.1504 - val_loss: 1.0666 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3937 - accuracy: 0.1498\n",
            "Epoch 95: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.3937 - accuracy: 0.1498 - val_loss: 1.0631 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.1493\n",
            "Epoch 96: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.3940 - accuracy: 0.1493 - val_loss: 1.0700 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.1531\n",
            "Epoch 97: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 961ms/step - loss: 0.3872 - accuracy: 0.1531 - val_loss: 1.0763 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.1512\n",
            "Epoch 98: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.3867 - accuracy: 0.1512 - val_loss: 1.0795 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.1523\n",
            "Epoch 99: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3822 - accuracy: 0.1523 - val_loss: 1.0618 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.1521\n",
            "Epoch 100: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.3794 - accuracy: 0.1521 - val_loss: 1.0638 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.1536\n",
            "Epoch 101: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3738 - accuracy: 0.1536 - val_loss: 1.0858 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.1516\n",
            "Epoch 102: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3754 - accuracy: 0.1516 - val_loss: 1.0705 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.1529\n",
            "Epoch 103: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3702 - accuracy: 0.1529 - val_loss: 1.0686 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.1529\n",
            "Epoch 104: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3741 - accuracy: 0.1529 - val_loss: 1.0710 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.1526\n",
            "Epoch 105: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.3681 - accuracy: 0.1526 - val_loss: 1.0786 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.1532\n",
            "Epoch 106: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3649 - accuracy: 0.1532 - val_loss: 1.0734 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.1544\n",
            "Epoch 107: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3617 - accuracy: 0.1544 - val_loss: 1.0735 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.1559\n",
            "Epoch 108: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3636 - accuracy: 0.1559 - val_loss: 1.0823 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.1558\n",
            "Epoch 109: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3591 - accuracy: 0.1558 - val_loss: 1.0723 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.1565\n",
            "Epoch 110: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3548 - accuracy: 0.1565 - val_loss: 1.0663 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.1549\n",
            "Epoch 111: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3570 - accuracy: 0.1549 - val_loss: 1.0724 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.1580\n",
            "Epoch 112: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3491 - accuracy: 0.1580 - val_loss: 1.0800 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.1542\n",
            "Epoch 113: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3521 - accuracy: 0.1542 - val_loss: 1.0726 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.1562\n",
            "Epoch 114: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.3488 - accuracy: 0.1562 - val_loss: 1.0704 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.1567\n",
            "Epoch 115: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3480 - accuracy: 0.1567 - val_loss: 1.0642 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.1564\n",
            "Epoch 116: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3492 - accuracy: 0.1564 - val_loss: 1.0766 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.1558\n",
            "Epoch 117: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3451 - accuracy: 0.1558 - val_loss: 1.0762 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.1585\n",
            "Epoch 118: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3362 - accuracy: 0.1585 - val_loss: 1.0681 - val_accuracy: 0.0647 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.1581\n",
            "Epoch 119: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3390 - accuracy: 0.1581 - val_loss: 1.0749 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.1566\n",
            "Epoch 120: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.3373 - accuracy: 0.1566 - val_loss: 1.0627 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.1600\n",
            "Epoch 121: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3339 - accuracy: 0.1600 - val_loss: 1.0733 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.1581\n",
            "Epoch 122: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3338 - accuracy: 0.1581 - val_loss: 1.0819 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.1583\n",
            "Epoch 123: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3274 - accuracy: 0.1583 - val_loss: 1.0772 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.1583\n",
            "Epoch 124: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.3279 - accuracy: 0.1583 - val_loss: 1.0779 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.1567\n",
            "Epoch 125: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3301 - accuracy: 0.1567 - val_loss: 1.0785 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.1585\n",
            "Epoch 126: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3278 - accuracy: 0.1585 - val_loss: 1.0686 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.1589\n",
            "Epoch 127: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3250 - accuracy: 0.1589 - val_loss: 1.0895 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.1600\n",
            "Epoch 128: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3229 - accuracy: 0.1600 - val_loss: 1.0694 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.1594\n",
            "Epoch 129: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3242 - accuracy: 0.1594 - val_loss: 1.0847 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.1587\n",
            "Epoch 130: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.3182 - accuracy: 0.1587 - val_loss: 1.0831 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.1613\n",
            "Epoch 131: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3186 - accuracy: 0.1613 - val_loss: 1.0827 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.1598\n",
            "Epoch 132: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3165 - accuracy: 0.1598 - val_loss: 1.0852 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.1609\n",
            "Epoch 133: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3140 - accuracy: 0.1609 - val_loss: 1.0837 - val_accuracy: 0.0642 - lr: 0.0010\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.1587\n",
            "Epoch 134: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.3183 - accuracy: 0.1587 - val_loss: 1.0859 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.1593\n",
            "Epoch 135: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3171 - accuracy: 0.1593 - val_loss: 1.0956 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.1592\n",
            "Epoch 136: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.3130 - accuracy: 0.1592 - val_loss: 1.0833 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.1593\n",
            "Epoch 137: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3139 - accuracy: 0.1593 - val_loss: 1.0932 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.1600\n",
            "Epoch 138: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3118 - accuracy: 0.1600 - val_loss: 1.0924 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.1592\n",
            "Epoch 139: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.3127 - accuracy: 0.1592 - val_loss: 1.0810 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.1600\n",
            "Epoch 140: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3100 - accuracy: 0.1600 - val_loss: 1.0978 - val_accuracy: 0.0647 - lr: 0.0010\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.1597\n",
            "Epoch 141: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3075 - accuracy: 0.1597 - val_loss: 1.0970 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.1618\n",
            "Epoch 142: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.3052 - accuracy: 0.1618 - val_loss: 1.0790 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.1620\n",
            "Epoch 143: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.3048 - accuracy: 0.1620 - val_loss: 1.0885 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.1614\n",
            "Epoch 144: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 971ms/step - loss: 0.3052 - accuracy: 0.1614 - val_loss: 1.1240 - val_accuracy: 0.0593 - lr: 0.0010\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.1609\n",
            "Epoch 145: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2999 - accuracy: 0.1609 - val_loss: 1.0969 - val_accuracy: 0.0647 - lr: 0.0010\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.1605\n",
            "Epoch 146: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.3008 - accuracy: 0.1605 - val_loss: 1.0911 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.1593\n",
            "Epoch 147: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.3012 - accuracy: 0.1593 - val_loss: 1.1005 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.1599\n",
            "Epoch 148: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2974 - accuracy: 0.1599 - val_loss: 1.0987 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.1613\n",
            "Epoch 149: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2960 - accuracy: 0.1613 - val_loss: 1.0965 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.1623\n",
            "Epoch 150: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2967 - accuracy: 0.1623 - val_loss: 1.1032 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.1636\n",
            "Epoch 151: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2964 - accuracy: 0.1636 - val_loss: 1.0987 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.1638\n",
            "Epoch 152: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2914 - accuracy: 0.1638 - val_loss: 1.0948 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.1603\n",
            "Epoch 153: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2944 - accuracy: 0.1603 - val_loss: 1.0971 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.1600\n",
            "Epoch 154: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2957 - accuracy: 0.1600 - val_loss: 1.1019 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.1632\n",
            "Epoch 155: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2882 - accuracy: 0.1632 - val_loss: 1.1090 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.1626\n",
            "Epoch 156: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2924 - accuracy: 0.1626 - val_loss: 1.0921 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.1635\n",
            "Epoch 157: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2857 - accuracy: 0.1635 - val_loss: 1.1061 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.1630\n",
            "Epoch 158: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2896 - accuracy: 0.1630 - val_loss: 1.0938 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.1636\n",
            "Epoch 159: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2852 - accuracy: 0.1636 - val_loss: 1.1079 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.1610\n",
            "Epoch 160: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2849 - accuracy: 0.1610 - val_loss: 1.0976 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.1635\n",
            "Epoch 161: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.2822 - accuracy: 0.1635 - val_loss: 1.1016 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.1638\n",
            "Epoch 162: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2797 - accuracy: 0.1638 - val_loss: 1.1139 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.1619\n",
            "Epoch 163: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2850 - accuracy: 0.1619 - val_loss: 1.1058 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.1652\n",
            "Epoch 164: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2759 - accuracy: 0.1652 - val_loss: 1.1115 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.1634\n",
            "Epoch 165: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.2762 - accuracy: 0.1634 - val_loss: 1.1085 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.1643\n",
            "Epoch 166: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2759 - accuracy: 0.1643 - val_loss: 1.1058 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.1634\n",
            "Epoch 167: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2784 - accuracy: 0.1634 - val_loss: 1.1060 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.1638\n",
            "Epoch 168: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2741 - accuracy: 0.1638 - val_loss: 1.1123 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.1627\n",
            "Epoch 169: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2744 - accuracy: 0.1627 - val_loss: 1.1162 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.1645\n",
            "Epoch 170: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.2737 - accuracy: 0.1645 - val_loss: 1.1252 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.1640\n",
            "Epoch 171: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2712 - accuracy: 0.1640 - val_loss: 1.1222 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.1642\n",
            "Epoch 172: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2697 - accuracy: 0.1642 - val_loss: 1.1175 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.1646\n",
            "Epoch 173: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2691 - accuracy: 0.1646 - val_loss: 1.1104 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.1640\n",
            "Epoch 174: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2689 - accuracy: 0.1640 - val_loss: 1.1152 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.1637\n",
            "Epoch 175: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2661 - accuracy: 0.1637 - val_loss: 1.1213 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.1641\n",
            "Epoch 176: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2680 - accuracy: 0.1641 - val_loss: 1.1267 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.1646\n",
            "Epoch 177: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2699 - accuracy: 0.1646 - val_loss: 1.1193 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.1640\n",
            "Epoch 178: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 963ms/step - loss: 0.2673 - accuracy: 0.1640 - val_loss: 1.1280 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.1640\n",
            "Epoch 179: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2658 - accuracy: 0.1640 - val_loss: 1.1334 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.1636\n",
            "Epoch 180: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2680 - accuracy: 0.1636 - val_loss: 1.1229 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.1636\n",
            "Epoch 181: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2644 - accuracy: 0.1636 - val_loss: 1.1276 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.1642\n",
            "Epoch 182: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 972ms/step - loss: 0.2621 - accuracy: 0.1642 - val_loss: 1.1289 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.1646\n",
            "Epoch 183: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 970ms/step - loss: 0.2635 - accuracy: 0.1646 - val_loss: 1.1184 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.1659\n",
            "Epoch 184: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2601 - accuracy: 0.1659 - val_loss: 1.1232 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.1641\n",
            "Epoch 185: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2623 - accuracy: 0.1641 - val_loss: 1.1349 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.1638\n",
            "Epoch 186: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 964ms/step - loss: 0.2618 - accuracy: 0.1638 - val_loss: 1.1358 - val_accuracy: 0.0623 - lr: 0.0010\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.1652\n",
            "Epoch 187: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2585 - accuracy: 0.1652 - val_loss: 1.1304 - val_accuracy: 0.0598 - lr: 0.0010\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.1638\n",
            "Epoch 188: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.2616 - accuracy: 0.1638 - val_loss: 1.1202 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.1642\n",
            "Epoch 189: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2584 - accuracy: 0.1642 - val_loss: 1.1335 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.1664\n",
            "Epoch 190: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 962ms/step - loss: 0.2556 - accuracy: 0.1664 - val_loss: 1.1291 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.1656\n",
            "Epoch 191: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 958ms/step - loss: 0.2543 - accuracy: 0.1656 - val_loss: 1.1313 - val_accuracy: 0.0618 - lr: 0.0010\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.1659\n",
            "Epoch 192: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2536 - accuracy: 0.1659 - val_loss: 1.1445 - val_accuracy: 0.0603 - lr: 0.0010\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.1670\n",
            "Epoch 193: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 966ms/step - loss: 0.2543 - accuracy: 0.1670 - val_loss: 1.1402 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.1652\n",
            "Epoch 194: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2519 - accuracy: 0.1652 - val_loss: 1.1269 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.1654\n",
            "Epoch 195: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 965ms/step - loss: 0.2547 - accuracy: 0.1654 - val_loss: 1.1443 - val_accuracy: 0.0632 - lr: 0.0010\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.1643\n",
            "Epoch 196: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2529 - accuracy: 0.1643 - val_loss: 1.1356 - val_accuracy: 0.0637 - lr: 0.0010\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.1679\n",
            "Epoch 197: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2499 - accuracy: 0.1679 - val_loss: 1.1310 - val_accuracy: 0.0627 - lr: 0.0010\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.1656\n",
            "Epoch 198: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 969ms/step - loss: 0.2503 - accuracy: 0.1656 - val_loss: 1.1491 - val_accuracy: 0.0608 - lr: 0.0010\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.1665\n",
            "Epoch 199: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 967ms/step - loss: 0.2479 - accuracy: 0.1665 - val_loss: 1.1496 - val_accuracy: 0.0613 - lr: 0.0010\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.1657\n",
            "Epoch 200: val_loss did not improve from 0.90907\n",
            "24/24 [==============================] - 23s 968ms/step - loss: 0.2498 - accuracy: 0.1657 - val_loss: 1.1475 - val_accuracy: 0.0627 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "weight_file_path = project_dir + \"seq2seq200.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath=weight_file_path, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "reduce_alpha = ReduceLROnPlateau(monitor ='val_loss', factor = 0.2, patience = 1, min_lr = 0.001)\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=25)\n",
        "\n",
        "history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
        "                                    epochs=200,\n",
        "                                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches,\n",
        "                                    callbacks=[checkpoint,reduce_alpha])\n",
        "\n",
        "model.save_weights(weight_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "K_mG4YEgt9gR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191ef7f2-2984-44e6-8cc0-294af2eac09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  Beyonc Giselle Knowles-Carter (/bijnse/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "question:  When did Beyonce start becoming popular?\n",
            "{'guessed_answer': 'the 1990s', 'actual_answer': 'in the late 1990s'}\n",
            "\n",
            "\n",
            "context:  Beyonc Giselle Knowles-Carter (/bijnse/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "question:  What was the first album Beyonc released as a solo artist?\n",
            "{'guessed_answer': 'dangerously in love', 'actual_answer': 'Dangerously in Love'}\n",
            "\n",
            "\n",
            "context:  Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Dj Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyonc also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyonc took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyonc (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.\n",
            "question:  When did Destiny's Child end their group act?\n",
            "{'guessed_answer': 'june 2005', 'actual_answer': 'June 2005'}\n",
            "\n",
            "\n",
            "context:  A self-described \"modern-day feminist\", Beyonc creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\n",
            "question:  In which years did Time rate Beyonce in the 100 most influential people in the world?\n",
            "{'guessed_answer': '2013', 'actual_answer': '2013 and 2014'}\n",
            "\n",
            "\n",
            "context:  Beyonc Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (ne Beyinc), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyonc's name is a tribute to her mother's maiden name. Beyonc's younger sister Solange is also a singer and a former member of Destiny's Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyonc is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.\n",
            "question:  What company did Beyonc's father work for when she was a child?\n",
            "{'guessed_answer': 'xerox', 'actual_answer': 'Xerox'}\n",
            "\n",
            "\n",
            "context:  Beyonc attended St. Mary's Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyonc's interest in music and performing continued after winning a school talent show at age seven, singing John Lennon's \"Imagine\" to beat 15/16-year-olds. In fall of 1990, Beyonc enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school's choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyonc was also a member of the choir at St. John's United Methodist Church as a soloist for two years.\n",
            "question:  What type of school was Parker Elementary School?\n",
            "{'guessed_answer': 'music magnet', 'actual_answer': 'music magnet school'}\n",
            "\n",
            "\n",
            "context:  At age eight, Beyonc and childhood friend Kelly Rowland met LaTavia Roberson while in an audition for an all-girl entertainment group. They were placed into a group with three other girls as Girl's Tyme, and rapped and danced on the talent show circuit in Houston. After seeing the group, R&B producer Arne Frager brought them to his Northern California studio and placed them in Star Search, the largest talent show on national TV at the time. Girl's Tyme failed to win, and Beyonc later said the song they performed was not good. In 1995 Beyonc's father resigned from his job to manage the group. The move reduced Beyonc's family's income by half, and her parents were forced to move into separated apartments. Mathew cut the original line-up to four and the group continued performing as an opening act for other established R&B girl groups. The girls auditioned before record labels and were finally signed to Elektra Records, moving to Atlanta Records briefly to work on their first recording, only to be cut by the company. This put further strain on the family, and Beyonc's parents separated. On October 5, 1995, Dwayne Wiggins's Grass Roots Entertainment signed the group. In 1996, the girls began recording their debut album under an agreement with Sony Music, the Knowles family reunited, and shortly after, the group got a contract with Columbia Records.\n",
            "question:  What large record company recorded Beyonce's group's first album?\n",
            "{'guessed_answer': 'sony music', 'actual_answer': 'Sony Music'}\n",
            "\n",
            "\n",
            "context:  LeToya Luckett and Roberson became unhappy with Mathew's managing of the band and eventually were replaced by Farrah Franklin and Michelle Williams. Beyonc experienced depression following the split with Luckett and Roberson after being publicly blamed by the media, critics, and blogs for its cause. Her long-standing boyfriend left her at this time. The depression was so severe it lasted for a couple of years, during which she occasionally kept herself in her bedroom for days and refused to eat anything. Beyonc stated that she struggled to speak about her depression because Destiny's Child had just won their first Grammy Award and she feared no one would take her seriously. Beyonc would later speak of her mother as the person who helped her fight it. Franklin was dismissed, leaving just Beyonc, Rowland, and Williams.\n",
            "question:  What event caused Beyonce's depression?\n",
            "{'guessed_answer': 'split with luckett and', 'actual_answer': 'split with Luckett and Rober'}\n",
            "\n",
            "\n",
            "context:  The remaining band members recorded \"Independent Women Part I\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyonc landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \"Bootylicious\" and the title track, \"Survivor\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\n",
            "question:  Independent Women Part I was on which 2000 film's soundtrack?\n",
            "{'guessed_answer': \"charlie 's angels\", 'actual_answer': \"Charlie's Angels.\"}\n",
            "\n",
            "\n",
            "context:  In July 2002, Beyonc continued her acting career playing Foxxy Cleopatra alongside Mike Myers in the comedy film, Austin Powers in Goldmember, which spent its first weekend atop the US box office and grossed $73 million. Beyonc released \"Work It Out\" as the lead single from its soundtrack album which entered the top ten in the UK, Norway, and Belgium. In 2003, Beyonc starred opposite Cuba Gooding, Jr., in the musical comedy The Fighting Temptations as Lilly, a single mother whom Gooding's character falls in love with. The film received mixed reviews from critics but grossed $30 million in the U.S. Beyonc released \"Fighting Temptation\" as the lead single from the film's soundtrack album, with Missy Elliott, MC Lyte, and Free which was also used to promote the film. Another of Beyonc's contributions to the soundtrack, \"Summertime\", fared better on the US charts.\n",
            "question:  What film did Beyonce appear in with Mike Myers?\n",
            "{'guessed_answer': 'austin powers in', 'actual_answer': 'Austin Powers in Goldmember'}\n",
            "\n",
            "\n",
            "context:  Beyonc's first solo recording was a feature on Jay Z's \"'03 Bonnie & Clyde\" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album's lead single, \"Crazy in Love\", featuring Jay Z, became Beyonc's first number-one single as a solo artist in the US. The single \"Baby Boy\" also reached number one, and singles, \"Me, Myself and I\" and \"Naughty Girl\", both reached the top-five. The album earned Beyonc a then record-tying five awards at the 46th Annual Grammy Awards; Best Contemporary R&B Album, Best Female R&B Vocal Performance for \"Dangerously in Love 2\", Best R&B Song and Best Rap/Sung Collaboration for \"Crazy in Love\", and Best R&B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.\n",
            "question:  Beyonce's first album by herself was called what?\n",
            "{'guessed_answer': 'dangerously in love', 'actual_answer': 'Dangerously in Love'}\n",
            "\n",
            "\n",
            "context:  In November 2003, she embarked on the Dangerously in Love Tour in Europe and later toured alongside Missy Elliott and Alicia Keys for the Verizon Ladies First Tour in North America. On February 1, 2004, Beyonc performed the American national anthem at Super Bowl XXXVIII, at the Reliant Stadium in Houston, Texas. After the release of Dangerously in Love, Beyonc had planned to produce a follow-up album using several of the left-over tracks. However, this was put on hold so she could concentrate on recording Destiny Fulfilled, the final studio album by Destiny's Child. Released on November 15, 2004, in the US and peaking at number two on the Billboard 200, Destiny Fulfilled included the singles \"Lose My Breath\" and \"Soldier\", which reached the top five on the Billboard Hot 100 chart. Destiny's Child embarked on a worldwide concert tour, Destiny Fulfilled... and Lovin' It and during the last stop of their European tour, in Barcelona on June 11, 2005, Rowland announced that Destiny's Child would disband following the North American leg of the tour. The group released their first compilation album Number 1's on October 25, 2005, in the US and accepted a star on the Hollywood Walk of Fame in March 2006.\n",
            "question:  Destiny's Child's final album was named what?\n",
            "{'guessed_answer': 'sasha', 'actual_answer': 'Destiny Fulfilled'}\n",
            "\n",
            "\n",
            "context:  Beyonc's second solo album B'Day was released on September 5, 2006, in the US, to coincide with her twenty-fifth birthday. It sold 541,000 copies in its first week and debuted atop the Billboard 200, becoming Beyonc's second consecutive number-one album in the United States. The album's lead single \"Dj Vu\", featuring Jay Z, reached the top five on the Billboard Hot 100 chart. The second international single \"Irreplaceable\" was a commercial success worldwide, reaching number one in Australia, Hungary, Ireland, New Zealand and the United States. B'Day also produced three other singles; \"Ring the Alarm\", \"Get Me Bodied\", and \"Green Light\" (released in the United Kingdom only).\n",
            "question:  The lead single from the album was which song?\n",
            "{'guessed_answer': 'third', 'actual_answer': 'Dj Vu'}\n",
            "\n",
            "\n",
            "context:  Her first acting role of 2006 was in the comedy film The Pink Panther starring opposite Steve Martin, grossing $158.8 million at the box office worldwide. Her second film Dreamgirls, the film version of the 1981 Broadway musical loosely based on The Supremes, received acclaim from critics and grossed $154 million internationally. In it, she starred opposite Jennifer Hudson, Jamie Foxx, and Eddie Murphy playing a pop singer based on Diana Ross. To promote the film, Beyonc released \"Listen\" as the lead single from the soundtrack album. In April 2007, Beyonc embarked on The Beyonc Experience, her first worldwide concert tour, visiting 97 venues and grossed over $24 million.[note 1] Beyonc conducted pre-concert food donation drives during six major stops in conjunction with her pastor at St. John's and America's Second Harvest. At the same time, B'Day was re-released with five additional songs, including her duet with Shakira \"Beautiful Liar\".\n",
            "question:  What did Beyonce call her first concert tour?\n",
            "{'guessed_answer': 'the beyonc', 'actual_answer': 'The Beyonc Experience'}\n",
            "\n",
            "\n",
            "context:  Beyonc further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. Her performance in the film received praise from critics, and she garnered several nominations for her portrayal of James, including a Satellite Award nomination for Best Supporting Actress, and a NAACP Image Award nomination for Outstanding Supporting Actress. Beyonc donated her entire salary from the film to Phoenix House, an organization of rehabilitation centers for heroin addicts around the country. On January 20, 2009, Beyonc performed James' \"At Last\" at the First Couple's first inaugural ball. Beyonc starred opposite Ali Larter and Idris Elba in the thriller, Obsessed. She played Sharon Charles, a mother and wife who learns of a woman's obsessive behavior over her husband. Although the film received negative reviews from critics, the movie did well at the US box office, grossing $68 million$60 million more than Cadillac Recordson a budget of $20 million. The fight scene finale between Sharon and the character played by Ali Larter also won the 2010 MTV Movie Award for Best Fight.\n",
            "question:  What genre of film was the movie, Obsessed, in which Beyonce starred in?\n",
            "{'guessed_answer': 'thriller', 'actual_answer': 'thriller'}\n",
            "\n",
            "\n",
            "context:  At the 52nd Annual Grammy Awards, Beyonc received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyonc was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyonc and Gaga, tying them with Mariah Carey for most number-ones since the Nielsen Top 40 airplay chart launched in 1992. \"Telephone\" received a Grammy Award nomination for Best Pop Collaboration with Vocals.\n",
            "question:  In 2010, Beyonce worked with which other famous singer?\n",
            "{'guessed_answer': 'drake carey', 'actual_answer': 'Lady Gaga'}\n",
            "\n",
            "\n",
            "context:  Beyonc announced a hiatus from her music career in January 2010, heeding her mother's advice, \"to live life, to be inspired by things again\". During the break she and her father parted ways as business partners. Beyonc's musical break lasted nine months and saw her visit multiple European cities, the Great Wall of China, the Egyptian pyramids, Australia, English music festivals and various museums and ballet performances.\n",
            "question:  Which year did Beyonce and her father part business ways?\n",
            "{'guessed_answer': '2010', 'actual_answer': '2010'}\n",
            "\n",
            "\n",
            "context:  In 2011, documents obtained by WikiLeaks revealed that Beyonc was one of many entertainers who performed for the family of Libyan ruler Muammar Gaddafi. Rolling Stone reported that the music industry was urging them to return the money they earned for the concerts; a spokesperson for Beyonc later confirmed to The Huffington Post that she donated the money to the Clinton Bush Haiti Fund. Later that year she became the first solo female artist to headline the main Pyramid stage at the 2011 Glastonbury Festival in over twenty years, and was named the highest-paid performer in the world per minute.\n",
            "question:  Beyonce became the first female artist to perform solo in 20 years at which stage?\n",
            "{'guessed_answer': 'the 2011 glastonbury', 'actual_answer': 'the 2011 Glastonbury Festival'}\n",
            "\n",
            "\n",
            "context:  In 2011, documents obtained by WikiLeaks revealed that Beyonc was one of many entertainers who performed for the family of Libyan ruler Muammar Gaddafi. Rolling Stone reported that the music industry was urging them to return the money they earned for the concerts; a spokesperson for Beyonc later confirmed to The Huffington Post that she donated the money to the Clinton Bush Haiti Fund. Later that year she became the first solo female artist to headline the main Pyramid stage at the 2011 Glastonbury Festival in over twenty years, and was named the highest-paid performer in the world per minute.\n",
            "question:  Beyonc was the first female singer to headline what at the 2011 Glastonbury Festival?\n",
            "{'guessed_answer': 'pyramid stage', 'actual_answer': 'Pyramid stage'}\n",
            "\n",
            "\n",
            "context:  Her fourth studio album 4 was released on June 28, 2011 in the US. 4 sold 310,000 copies in its first week and debuted atop the Billboard 200 chart, giving Beyonc her fourth consecutive number-one album in the US. The album was preceded by two of its singles \"Run the World (Girls)\" and \"Best Thing I Never Had\", which both attained moderate success. The fourth single \"Love on Top\" was a commercial success in the US. 4 also produced four other singles; \"Party\", \"Countdown\", \"I Care\" and \"End of Time\". \"Eat, Play, Love\", a cover story written by Beyonc for Essence that detailed her 2010 career break, won her a writing award from the New York Association of Black Journalists. In late 2011, she took the stage at New York's Roseland Ballroom for four nights of special performances: the 4 Intimate Nights with Beyonc concerts saw the performance of her 4 album to a standing room only.\n",
            "question:  What magazine did Beyonc write a story for about her earlier hiatus?\n",
            "{'guessed_answer': 'essence', 'actual_answer': 'Essence'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "\n",
        "def reply(paragraph, question):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        input_text = paragraph.lower() + ' Q ' + question.lower()\n",
        "        for word in nltk.word_tokenize(input_text):\n",
        "            if word != 'Q' and (not in_white_list(word)):\n",
        "                continue\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in input_word2idx:\n",
        "                idx = input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, input_max_seq_length)\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, num_target_tokens))\n",
        "        target_seq[0, 0, target_word2idx['START']] = 1\n",
        "        target_text = ''\n",
        "        target_text_len = 0\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = target_idx2word[sample_token_idx]\n",
        "            target_text_len += 1\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or target_text_len >= target_max_seq_length:\n",
        "                terminated = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, num_target_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return target_text.strip()\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "    index = i * 10\n",
        "    paragraph, question, actual_answer = data_set1[index]\n",
        "    predicted_answer = reply(paragraph, question)\n",
        "    print('context: ', paragraph)\n",
        "    print('question: ', question)\n",
        "    print({'guessed_answer': predicted_answer, 'actual_answer': actual_answer})\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1cV-Be52ov_Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "69e4ab79-e179-46b7-d663-e4bf32896596"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4bf2ee6f5cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf1score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'true_values' is not defined"
          ]
        }
      ],
      "source": [
        "# precision, recall, f1-score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "precision = precision_score(true_values, predict_values)\n",
        "recall = recall_score(true_values, predict_values)\n",
        "f1score = f1_score(true_values, predict_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkiNvUAw7UsS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}